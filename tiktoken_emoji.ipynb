{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MagaliDrumare/Token-/blob/main/tiktoken_emoji.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziirFV9PaG-T",
        "outputId": "f1f4a09b-4acb-4c14-ce04-cc308cbf48b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji) (4.12.2)\n",
            "Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.12.1\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken\n",
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init the GPT-4 Tokenizer\n",
        "import tiktoken\n",
        "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "print(enc.n_vocab) # number of tokens in total"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57wUOMOhaL2y",
        "outputId": "895670ba-0108-4d56-cb13-601421265175"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init the emojis\n",
        "import emoji\n",
        "emojis = list(emoji.EMOJI_DATA.keys())\n",
        "import random\n",
        "random.seed(15)\n",
        "random.shuffle(emojis)\n",
        "print(len(emoji.EMOJI_DATA)) # number of possible emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IODCjlnLeNjh",
        "outputId": "3486a451-28f8-4f9b-9acb-c5c60655c723"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_tokens(text, max_per_row=10):\n",
        "    ids = enc.encode(text)\n",
        "    unique_tokens = set(ids)\n",
        "    # map all tokens we see to a unique emoji\n",
        "    id_to_emoji = {id: emoji for emoji, id in zip(emojis, unique_tokens)}\n",
        "    # do the translatation\n",
        "    lines = []\n",
        "    for i in range(0, len(ids), max_per_row):\n",
        "        lines.append(''.join([id_to_emoji[id] for id in ids[i:i+max_per_row]]))\n",
        "    out = '\\n'.join(lines)\n",
        "    return out"
      ],
      "metadata": {
        "id": "KvSEuUZreWms"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Words vs Tokens\n",
        "A word is most likely what you think it is - the most simple form or unit of language as understood by humans. In the sentence, “I like cats”, there are three words - “I”, “like”, and “cats.” We can think of words as the primary building blocks of language; the fundamental pieces of language that we are taught from a very young age.\n",
        "\n",
        "A token is a bit more complex. Tokenization is the process of converting pieces of language into bits of data that are usable for a program, and a tokenizer is an algorithm or function that performs this process, i.e., takes language and converts it into these usable bits of data. Thus, a token is a unit of text that is intentionally segmented for a large language model to process efficiently. These units can be words or any other subset of language - parts of words, combinations of words, or punctuation.\n",
        "\n",
        "There are a variety of different tokenizers out there which reflect a variety of trade offs. Well-known tokenizers include NLTK (Natural Language Toolkit), Spacy, BERT tokenizer and Keras. Whether or not to select one of these or a different tokenizer depends upon your specific use case. On average, there are roughly 0.75 words per token, but there can be meaningful differences among tokenizers.\"\"\"\n",
        "\n",
        "print(text_to_tokens(text, max_per_row=15))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjCMId2KaPWG",
        "outputId": "34b4017d-ff2c-450b-97eb-b3056eb8292d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧍🏿🏃🏾‍➡👩🏻‍🤝‍👩🏼🕴🏾🏋🏿🚶🏼‍♂️‍➡🏋🏾‍♀🇼🇸🤝🏾❎🫷👩🏾‍🦯🖖🏾🏋🏾‍♀👨🏻‍🦲\n",
            "🏌‍♂️🇼🇸👳‍♂️🧑🏽‍❤‍🧑🏾🫁🧑🏽‍🤝‍🧑🏽🎗️🔓👱‍♀👩🏻‍❤️‍💋‍👨🏾🪱🧵🤾🏻‍♀️🧑🏽‍⚖🏌‍♂️\n",
            "🔣🧎‍♀🧙🏽‍♀️👮🏾🇪🇷🤙🏻🧑🏼‍❤️‍💋‍🧑🏾🤎🍏👨‍👩‍👦🧑🏼‍🏫👨🏻‍🦲🧙🏽‍♀️👮🏾🧑🏼‍❤️‍💋‍🧑🏾\n",
            "🧙🏽‍♀️🧑🏾‍🦯‍➡🧑🏼‍❤️‍💋‍🧑🏾👨🏾‍🦲🧙🏽‍♀️🍱🧚🏻‍♀️👩🏻‍❤‍💋‍👩🏾🇳🇫👩🏾‍🦯🎗️🧑🏼‍🏫👱‍♀🏌‍♂️📏\n",
            "👩🏻‍🦲↕️🎗️🔓🧑‍🦼‍➡️🏌‍♂️👨🏿‍❤️‍💋‍👨🏻👩🏿‍❤️‍👩🏻🎗️🔓🇷🇪🇨🇦🍏👁️‍🗨️🏃🏽‍♂️‍➡\n",
            "🖲☪️👱🏾‍♂🚶🏾‍♀‍➡🤫🏋🏿🇬🇹🏋🏾‍♀🖲🌫🧑🏻‍🦯‍➡️🎅🏼🤾🏻‍♀️🏄🏾‍♂✌🏿\n",
            "🏋🏾‍♀🏌‍♂️🔃🎗️🧑🏾‍❤‍💋‍🧑🏿👩🏿‍❤️‍👩🏻🎗️🔓🥘🧑🏻‍🎓🎗️📮🇷🇪🍏🕵🏼‍♀️\n",
            "☺🖲🈴🧎‍♀👨🏾‍🦲🖲📕🏋🏾‍♀🌶🫱🫁🇫🇷🇷🇪😕👩‍🦽‍➡\n",
            "🔃🧎‍♀🦸🏾‍♂️🧑🏽‍⚕🤸🏾‍♂⏰🔓👨🏾‍🦲🧔🏼🖖🏾🥘🏊🕵🏼‍♀️🧑🏻‍🎓🎗️\n",
            "📮🤾🏻‍♀️💇🏾🧎‍♀🖲🇬🇹🏋🏾‍♀🖲🧑🏽‍🤝‍🧑🏽🎗️🚶🏻‍♀‍➡🇷🇪🏋🏾‍♀🧔🏽‍♀👩🏿‍❤️‍💋‍👨🏻\n",
            "☺🖲🏌🔓🧎🏾‍♀‍➡🇰🇾🔃〰️🤾🏻‍♀️👨🏽‍❤‍👨🏽🗜️🇳🇫🕓🧑🏼‍🏫🫁\n",
            "🥖🏄🏼💆🏽‍♀️🎗️🔓👨🏻‍🦲🤲🏿🎗️🧑🏼‍🏫🧎‍♀♀️🎗️🧑🏼‍🏫🧎‍♀🫁\n",
            "🧑‍🍼🤫🇦🇫🍏🖲🧑🏾‍🦼‍➡️🎗️🌭🇬🇹♑🔶🤎🛸🧑🏼‍🦽‍➡️🖲\n",
            "🧑🏾‍🦼‍➡️🎗️🤾‍♀🌱🤾🏻‍♀️👔🧗🏼‍♀️🇬🇹♑🇵🇾🇹🇯👩🏿‍🎓📚🙋🏻‍♀🏾\n",
            "🙇🏿‍♂️📆🧚🏿‍♂✋🏼🧎‍♀👨🏽‍🌾🤸🏼‍♀️📕👨🏾‍🦲👨‍🦽‍➡️🧗🏾‍♂🤾🏻‍♀️🇳🇨🫁🧑‍🦽\n",
            "🇰🇾⏳🤵🏼‍♀️🎗️🏊🫁🖲🌭📕👨‍🍼🗣👩🏿‍🦯🪖🏃🏿‍♀‍➡️🧗🏻‍♀\n",
            "🤾🏻‍♀️🖊🦹🏼‍♂️🧎‍♀🤎🍏🚶🏿‍♂️🤵‍♀🙍‍♀️🤾🏻‍♀️💂🧑🏼‍🏫🪤🇬🇹🧎‍♀\n",
            "💌🤎🇳🇫🕓⬆️👩🏾‍🦳🙅‍♀🇬🇹♑🤾🏻‍♀️\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Quelle est la capitale de la France?\"\"\"\n",
        "print(text_to_tokens(text, max_per_row=20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75OlT3yhf9p5",
        "outputId": "8729cdc3-b622-4f1a-e06d-cdc06e9662fc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✌🏿🤾🏻‍♀️🧎‍♀🈴🧑🏾‍🦼‍➡️📏🤙🏻🈴🍏🙍‍♀️\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc.encode(text)"
      ],
      "metadata": {
        "id": "66mI2sQ-he0E",
        "outputId": "1ac2a71d-b576-47c1-bc46-5ec3b237d19e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2232, 6853, 1826, 1208, 61510, 1604, 409, 1208, 9822, 30]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Quelle est la capitale de l'Allemagne?\"\"\"\n",
        "print(text_to_tokens(text, max_per_row=20))"
      ],
      "metadata": {
        "id": "aqcySh35hYES",
        "outputId": "f76f525b-4c88-4f12-9d25-cd65b4111bf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧑‍🦼‍➡️🤾🏻‍♀️🧎‍♀👩🏿‍❤️‍💋‍👨🏻🧑🏾‍🦼‍➡️📏🧑‍🦽🍏🙍‍♀️✌🏿💂🤙🏻🈴\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMHXKsahgBg-",
        "outputId": "6c608f8b-6727-49ec-be71-bb3f4fb67ed9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2232, 6853, 1826, 1208, 61510, 1604, 409, 326, 6, 2149, 3516, 24812, 30]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}